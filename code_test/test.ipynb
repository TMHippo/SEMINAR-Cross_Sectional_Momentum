{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "data = yf.download('AAPL', start='2020-01-01', end='2023-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Daily Return'] = data['Adj Close'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_daily_return = data['Daily Return'].mean()\n",
    "std_daily_return = data['Daily Return'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_free_rate = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annual Sharpe Ratio: 0.5632049743217762\n"
     ]
    }
   ],
   "source": [
    "sharpe_ratio = (avg_daily_return - risk_free_rate / 252) / std_daily_return\n",
    "annual_sharpe_ratio = sharpe_ratio * np.sqrt(252)\n",
    "print(f'Annual Sharpe Ratio: {annual_sharpe_ratio}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CROSS SECTIONAL MOMENTUM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  7 of 7 completed\n"
     ]
    }
   ],
   "source": [
    "# Tải dữ liệu của nhiều cổ phiếu (có thể thay đổi danh sách này)\n",
    "tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'NFLX', 'META']\n",
    "\n",
    "data = yf.download(tickers, start='2020-01-01', end='2024-01-01')['Adj Close']\n",
    "\n",
    "# Tính toán lợi suất hàng ngày (daily returns)\n",
    "returns = data.pct_change().dropna()\n",
    "\n",
    "# Tính hiệu suất của mỗi cổ phiếu trong 12 tháng trước đó (252 ngày giao dịch)\n",
    "lookback_period = 252\n",
    "momentum_returns = returns.rolling(window=lookback_period).apply(np.sum)\n",
    "\n",
    "# Lấy dữ liệu hiệu suất của tháng gần nhất để xếp hạng\n",
    "momentum_last_month = momentum_returns.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winners (Buy): Index(['META', 'TSLA', 'AMZN'], dtype='object', name='Ticker')\n",
      "Losers (Sell): Index(['GOOGL', 'MSFT', 'AAPL'], dtype='object', name='Ticker')\n"
     ]
    }
   ],
   "source": [
    "# Xếp hạng các cổ phiếu dựa trên lợi suất trong 12 tháng trước đó\n",
    "top_n = 3  # Số cổ phiếu có hiệu suất tốt nhất\n",
    "bottom_n = 3  # Số cổ phiếu có hiệu suất kém nhất\n",
    "\n",
    "# Sắp xếp các cổ phiếu dựa trên hiệu suất\n",
    "ranked_stocks = momentum_last_month.sort_values(ascending=False)\n",
    "\n",
    "# Lựa chọn cổ phiếu tốt nhất và kém nhất\n",
    "winners = ranked_stocks.head(top_n).index  # Mua các cổ phiếu tốt nhất\n",
    "losers = ranked_stocks.tail(bottom_n).index  # Bán các cổ phiếu kém nhất\n",
    "\n",
    "print(f\"Winners (Buy): {winners}\")\n",
    "print(f\"Losers (Sell): {losers}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lợi suất chiến lược cumulative return:\n",
      "Date\n",
      "2023-11-30 00:00:00+00:00    0.993866\n",
      "2023-12-01 00:00:00+00:00    0.995186\n",
      "2023-12-04 00:00:00+00:00    0.995240\n",
      "2023-12-05 00:00:00+00:00    0.988102\n",
      "2023-12-06 00:00:00+00:00    0.990439\n",
      "2023-12-07 00:00:00+00:00    0.987027\n",
      "2023-12-08 00:00:00+00:00    0.995383\n",
      "2023-12-11 00:00:00+00:00    0.989978\n",
      "2023-12-12 00:00:00+00:00    0.995441\n",
      "2023-12-13 00:00:00+00:00    0.996555\n",
      "2023-12-14 00:00:00+00:00    1.016969\n",
      "2023-12-15 00:00:00+00:00    1.022703\n",
      "2023-12-18 00:00:00+00:00    1.032893\n",
      "2023-12-19 00:00:00+00:00    1.040458\n",
      "2023-12-20 00:00:00+00:00    1.023911\n",
      "2023-12-21 00:00:00+00:00    1.035077\n",
      "2023-12-22 00:00:00+00:00    1.029120\n",
      "2023-12-26 00:00:00+00:00    1.036852\n",
      "2023-12-27 00:00:00+00:00    1.049296\n",
      "2023-12-28 00:00:00+00:00    1.037254\n",
      "2023-12-29 00:00:00+00:00    1.025889\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Tính lợi suất của nhóm cổ phiếu thắng (winners) và thua (losers) trong tháng tiếp theo\n",
    "future_returns = returns.iloc[-21:]  # Lợi suất trong tháng tiếp theo (21 ngày giao dịch)\n",
    "\n",
    "# Lợi nhuận trung bình của các cổ phiếu mua và bán\n",
    "winners_return = future_returns[winners].mean(axis=1)\n",
    "losers_return = future_returns[losers].mean(axis=1)\n",
    "\n",
    "# Lợi nhuận của chiến lược cross-sectional momentum\n",
    "momentum_strategy_return = winners_return - losers_return\n",
    "\n",
    "# Tổng lợi suất chiến lược\n",
    "cumulative_return = (1 + momentum_strategy_return).cumprod()\n",
    "\n",
    "print(f\"Lợi suất chiến lược cumulative return:\\n{cumulative_return}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharpe Ratio: Ticker\n",
      "AAPL     0.055250\n",
      "AMZN     0.030814\n",
      "GOOGL    0.043328\n",
      "META     0.032057\n",
      "MSFT     0.052388\n",
      "NFLX     0.027990\n",
      "TSLA     0.071127\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Giả sử lãi suất phi rủi ro là 0.02/năm (tính ra hàng ngày là 0.02/252)\n",
    "risk_free_rate = 0.02/ (3 * 365)\n",
    "\n",
    "# Tính Sharpe Ratio\n",
    "excess_returns = returns - risk_free_rate  # Lợi suất vượt mức\n",
    "mean_excess_return = excess_returns.mean()  # Lợi suất vượt mức trung bình\n",
    "std_excess_return = excess_returns.std()  # Độ lệch chuẩn của lợi suất vượt mức\n",
    "\n",
    "sharpe_ratio = mean_excess_return / std_excess_return\n",
    "print(f\"Sharpe Ratio: {sharpe_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "def to_timestamp(date_string):\n",
    "    return int(datetime.strptime(date_string, \"%d-%m-%Y %H:%M\").timestamp())\n",
    "\n",
    "# converts a timestamp to UTC date in the format %d-%m-%Y %H:%M\n",
    "def to_utc_date(timestamp):\n",
    "    return datetime.utcfromtimestamp(int(timestamp)).strftime('%d-%m-%Y %H:%M')\n",
    "\n",
    "def current_timestamp():\n",
    "    return int(datetime.now().timestamp())\n",
    "\n",
    "def progressBar(iterable, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█', printEnd = \"\\r\"):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
    "    \"\"\"\n",
    "    total = len(iterable)\n",
    "    # Progress Bar Printing Function\n",
    "    def printProgressBar (iteration):\n",
    "        percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "        filledLength = int(length * iteration // total)\n",
    "        bar = fill * filledLength + '-' * (length - filledLength)\n",
    "        print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end = printEnd)\n",
    "    # Initial Call\n",
    "    printProgressBar(0)\n",
    "    # Update Progress Bar\n",
    "    for i, item in enumerate(iterable):\n",
    "        yield item\n",
    "        printProgressBar(i + 1)\n",
    "    # Print New Line on Complete\n",
    "    print()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start = '01-09-2020 00:00'\n",
    "    end = None #A valid date or None for the current date\n",
    "\n",
    "    currency_pair = 'BTCUSD'\n",
    "    pair_sufix = 'inverse'\n",
    "    interval = 1 #valid values are: 1,3,5,15,30\n",
    "    wait_count = 3\n",
    "\n",
    "\n",
    "    start_date = to_timestamp(start)\n",
    "    end_date = current_timestamp() if end == None else to_timestamp(end)\n",
    "    if end == None:\n",
    "        end = to_utc_date(end_date)\n",
    "    filename = f'BYBIT-{currency_pair}-{pair_sufix}-{interval}m-data-from-{start.replace(\" \", \"_\")}-to-{end.replace(\" \", \"_\")}.csv'\n",
    "\n",
    "\n",
    "    print(f'Downloading dataset {currency_pair} {pair_sufix} in candles of {interval}m from {start} to {end}')\n",
    "    print(\"total number of candles is\",int((int(end_date)-int(start_date))/ ( 60 * interval)))\n",
    "\n",
    "    list_candle_times = list(range(start_date, end_date, int(200 * 60 * interval)))\n",
    "    data = []\n",
    "\n",
    "    for current_time in progressBar(list_candle_times, prefix = 'Progress:', suffix = 'Complete', length = 50):\n",
    "        status_code = 0\n",
    "        response = 1\n",
    "\n",
    "        while(status_code != 200):\n",
    "            response = requests.get(f'https://api.bybit.com/v2/public/kline/list?symbol={currency_pair}&interval={interval}&limit=200&from={current_time}')\n",
    "            status_code = response.status_code\n",
    "            time.sleep(0.1)\n",
    "\n",
    "        for candle in response.json()[\"result\"]:\n",
    "            if end_date <= candle[\"open_time\"]:\n",
    "                break\n",
    "            data.append([candle[\"open_time\"], candle[\"open\"], candle[\"high\"],candle[\"low\"],candle[\"close\"],candle[\"volume\"], int(candle[\"open_time\"]) + interval * 60])\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume', 'close_time'])\n",
    "\n",
    "    # Saving the CSV\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    df.to_csv(filename, sep=\",\", encoding='utf-8', index='timestamp')\n",
    "\n",
    "    print(\"Download finished !!!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ryuryu's Bybit Historical Data Downloader \n",
    "# (Production Mode 6973)\n",
    "# -------------------------------------\n",
    "# (c) 2022 Ryan Hayabusa \n",
    "# Github: https://github.com/ryu878 \n",
    "# Web: https://aadresearch.xyz/\n",
    "# Discord: ryuryu#4087\n",
    "# -------------------------------------\n",
    "# pip install beautifulsoup4\n",
    "# pip install requests\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "import re\n",
    "import gzip\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# Set the file version\n",
    "ver = '1.3:02/05/23'\n",
    "\n",
    "# Define the base URL\n",
    "base_url = 'https://public.bybit.com/trading/'\n",
    "\n",
    "# Select the start date\n",
    "start_date = '2023-05-01'\n",
    "\n",
    "# Set the list of coins\n",
    "coins = []\n",
    "\n",
    "# Create a function to download the files\n",
    "def download_file(url, local_path):\n",
    "    with urllib.request.urlopen(url) as response, open(local_path, 'wb') as out_file:\n",
    "        data = response.read()\n",
    "        out_file.write(data)\n",
    "\n",
    "\n",
    "# Create a function to check if a file exists\n",
    "def file_exists(local_path):\n",
    "    return os.path.exists(local_path)\n",
    "\n",
    "\n",
    "# Make a GET request to the base URL and parse the HTML\n",
    "response = requests.get(base_url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all the links on the page\n",
    "links = soup.find_all('a')\n",
    "\n",
    "# Loop through all the links\n",
    "for link in links:\n",
    "    # Get the href attribute of the link\n",
    "    href = link.get('href')\n",
    "    # Check if the href attribute is a directory\n",
    "    if href.endswith('/'):\n",
    "        # Get the directory name\n",
    "        dir_name = href[:-1]\n",
    "        # Create the directory locally if it doesn't exist\n",
    "        if not os.path.exists(dir_name):\n",
    "            os.mkdir(dir_name)\n",
    "        # Make a GET request to the directory URL and parse the HTML\n",
    "        dir_url = base_url + href\n",
    "        dir_response = requests.get(dir_url)\n",
    "        dir_soup = BeautifulSoup(dir_response.text, 'html.parser')\n",
    "        # Find all the CSV files in the directory\n",
    "        csv_links = dir_soup.find_all(href=re.compile('.csv.gz$'))\n",
    "        # Loop through all the CSV files\n",
    "        for csv_link in csv_links:\n",
    "            # Get the CSV file name\n",
    "            csv_name = csv_link.text\n",
    "            # Extract the date from the CSV file name\n",
    "            csv_date = re.findall(r'\\d{4}-\\d{2}-\\d{2}', csv_name)[0]\n",
    "            # Check if the file is from or after the selected start date\n",
    "            if csv_date >= start_date:\n",
    "                # Construct the full URL of the CSV file\n",
    "                csv_url = dir_url + csv_name\n",
    "                # Construct the local path of the extracted file\n",
    "                extracted_path = os.path.join(dir_name, csv_name[:-3])\n",
    "                # Check if the extracted file exists locally\n",
    "                if file_exists(extracted_path):\n",
    "                    print('Skipping download of', csv_name, '- extracted file already exists.')\n",
    "                else:\n",
    "                    # Construct the local path of the archive file\n",
    "                    archive_path = os.path.join(dir_name, csv_name)\n",
    "                    # Download the archive file if it doesn't exist locally\n",
    "                    if not file_exists(archive_path):\n",
    "                        download_file(csv_url, archive_path)\n",
    "                        print('Downloaded:', archive_path)\n",
    "                        time.sleep(0.1)\n",
    "                    # Check if the file is a gzip archive\n",
    "                    if csv_name.endswith('.gz'):\n",
    "                        # Open the gzip archive and extract the contents\n",
    "                        with gzip.open(archive_path, 'rb') as f_in:\n",
    "                            with open(extracted_path, 'wb') as f_out:\n",
    "                                f_out.write(f_in.read())\n",
    "                                print('Extracted:', extracted_path)\n",
    "                        # Remove the archive file\n",
    "                        os.remove(archive_path)\n",
    "                        print('Removed:', archive_path)\n",
    "                    else:\n",
    "                        # Rename the file to remove the .csv extension\n",
    "                        os.rename(archive_path, extracted_path)\n",
    "                        print('Renamed:', archive_path, 'to', extracted_path)\n",
    "            else:\n",
    "                # Skip the file\n",
    "                print('Skipping download of', csv_name, '- date is before start date.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

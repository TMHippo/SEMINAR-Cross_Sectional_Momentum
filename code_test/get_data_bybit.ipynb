{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ryuryu's Bybit Historical Data Downloader \n",
    "# (Production Mode 6973)\n",
    "# -------------------------------------\n",
    "# (c) 2022 Ryan Hayabusa \n",
    "# Github: https://github.com/ryu878 \n",
    "# Web: https://aadresearch.xyz/\n",
    "# Discord: ryuryu#4087\n",
    "# -------------------------------------\n",
    "# pip install beautifulsoup4\n",
    "# pip install requests\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "import re\n",
    "import gzip\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# Set the file version\n",
    "ver = '1.3:02/05/23'\n",
    "# Define the base URL\n",
    "base_url = 'https://public.bybit.com/kline_for_metatrader4/'\n",
    "# Select the start date\n",
    "start_date = '2024-10-10'\n",
    "\n",
    "# Set the list of coins\n",
    "coins = []\n",
    "\n",
    "# Create a function to download the files\n",
    "def download_file(url, local_path):\n",
    "    with urllib.request.urlopen(url) as response, open(local_path, 'wb') as out_file:\n",
    "        data = response.read()\n",
    "        out_file.write(data)\n",
    "\n",
    "\n",
    "# Create a function to check if a file exists\n",
    "def file_exists(local_path):\n",
    "    return os.path.exists(local_path)\n",
    "\n",
    "\n",
    "# Make a GET request to the base URL and parse the HTML\n",
    "response = requests.get(base_url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all the links on the page\n",
    "texts = ['BTCUSD/', 'AVAXUSDT/', 'AXSUSDT/', 'BNBUSDT/','ETCUSDT/']\n",
    "links = soup.find_all('a', string=lambda text: text in texts)\n",
    "# 'BTCUSD/', 'BTCUSDT/', \n",
    "\n",
    "# Loop through all the links\n",
    "for link in links:\n",
    "    # Get the href attribute of the link\n",
    "    href = link.get('href')\n",
    "    # Check if the href attribute is a directory\n",
    "    if href.endswith('/'):\n",
    "        # Get the directory name\n",
    "        dir_name = href[:-1]\n",
    "        # Create the directory locally if it doesn't exist\n",
    "        if not os.path.exists(dir_name):\n",
    "            os.mkdir(dir_name)\n",
    "        # Make a GET request to the directory URL and parse the HTML\n",
    "        dir_url = base_url + href\n",
    "        dir_response = requests.get(dir_url)\n",
    "        dir_soup = BeautifulSoup(dir_response.text, 'html.parser')\n",
    "        # Find all the CSV files in the directory\n",
    "        csv_links = dir_soup.find_all(href=re.compile('.csv.gz$'))\n",
    "        # Loop through all the CSV files\n",
    "        for csv_link in csv_links:\n",
    "            # Get the CSV file name\n",
    "            csv_name = csv_link.text\n",
    "            # Extract the date from the CSV file name\n",
    "            csv_date = re.findall(r'\\d{4}-\\d{2}-\\d{2}', csv_name)[0]\n",
    "            # Check if the file is from or after the selected start date\n",
    "            if csv_date >= start_date:\n",
    "                # Construct the full URL of the CSV file\n",
    "                csv_url = dir_url + csv_name\n",
    "                # Construct the local path of the extracted file\n",
    "                extracted_path = os.path.join(dir_name, csv_name[:-3])\n",
    "                # Check if the extracted file exists locally\n",
    "                if file_exists(extracted_path):\n",
    "                    print('Skipping download of', csv_name, '- extracted file already exists.')\n",
    "                else:\n",
    "                    # Construct the local path of the archive file\n",
    "                    archive_path = os.path.join(dir_name, csv_name)\n",
    "                    # Download the archive file if it doesn't exist locally\n",
    "                    if not file_exists(archive_path):\n",
    "                        download_file(csv_url, archive_path)\n",
    "                        print('Downloaded:', archive_path)\n",
    "                        time.sleep(0.1)\n",
    "                    # Check if the file is a gzip archive\n",
    "                    if csv_name.endswith('.gz'):\n",
    "                        # Open the gzip archive and extract the contents\n",
    "                        with gzip.open(archive_path, 'rb') as f_in:\n",
    "                            with open(extracted_path, 'wb') as f_out:\n",
    "                                f_out.write(f_in.read())\n",
    "                                print('Extracted:', extracted_path)\n",
    "                        # Remove the archive file\n",
    "                        os.remove(archive_path)\n",
    "                        print('Removed:', archive_path)\n",
    "                    else:\n",
    "                        # Rename the file to remove the .csv extension\n",
    "                        os.rename(archive_path, extracted_path)\n",
    "                        print('Renamed:', archive_path, 'to', extracted_path)\n",
    "            else:\n",
    "                # Skip the file\n",
    "                print('Skipping download of', csv_name, '- date is before start date.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "import re\n",
    "import gzip\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set the file version\n",
    "ver = '1.5:23/10/23'  # Cập nhật phiên bản\n",
    "# Define the base URL\n",
    "base_url = 'https://public.bybit.com/kline_for_metatrader4/'\n",
    "# Select the start and end dates\n",
    "start_date = '2023-1-1'\n",
    "end_date = '2024-6-30'  # Ngày kết thúc\n",
    "interval = 60  # Khoảng thời gian theo phút\n",
    "\n",
    "# Set the list of coins\n",
    "coins = []\n",
    "\n",
    "# Create a function to download the files\n",
    "def download_file(url, local_path):\n",
    "    with urllib.request.urlopen(url) as response, open(local_path, 'wb') as out_file:\n",
    "        data = response.read()\n",
    "        out_file.write(data)\n",
    "\n",
    "\n",
    "# Create a function to check if a file exists\n",
    "def file_exists(local_path):\n",
    "    return os.path.exists(local_path)\n",
    "\n",
    "\n",
    "# Make a GET request to the base URL and parse the HTML\n",
    "response = requests.get(base_url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all the links on the page\n",
    "texts = ['BTCUSD/', 'AVAXUSDT/', 'AXSUSDT/', 'BNBUSDT/','ETCUSDT/']\n",
    "links = soup.find_all('a', string=lambda text: text in texts)\n",
    "# 'BTCUSD/', 'BTCUSDT/', \n",
    "\n",
    "# Loop through all the links\n",
    "for link in links:\n",
    "    # Get the href attribute of the link\n",
    "    href = link.get('href')\n",
    "    # Check if the href attribute is a directory\n",
    "    if href.endswith('/'):\n",
    "        # Get the directory name\n",
    "        dir_name = href[:-1]\n",
    "        # Create the directory locally if it doesn't exist\n",
    "        if not os.path.exists(dir_name):\n",
    "            os.mkdir(dir_name)\n",
    "        # Make a GET request to the directory URL and parse the HTML\n",
    "        dir_url = base_url + href\n",
    "        dir_response = requests.get(dir_url)\n",
    "        dir_soup = BeautifulSoup(dir_response.text, 'html.parser')\n",
    "        # Find all the CSV files in the directory\n",
    "        csv_links = dir_soup.find_all(href=re.compile('.csv.gz$'))\n",
    "        # Loop through all the CSV files\n",
    "        for csv_link in csv_links:\n",
    "            # Get the CSV file name\n",
    "            csv_name = csv_link.text\n",
    "            # Extract the date from the CSV file name\n",
    "            csv_date = re.findall(r'\\d{4}-\\d{2}-\\d{2}', csv_name)[0]\n",
    "            # Extract the interval from the CSV file name (giả sử nó chứa \"60m\" cho 60 phút)\n",
    "            if f'{interval}m' not in csv_name:\n",
    "                print('Skipping download of', csv_name, '- interval does not match.')\n",
    "                continue\n",
    "            # Check if the file is within the start and end date range\n",
    "            if start_date <= csv_date <= end_date:\n",
    "                # Construct the full URL of the CSV file\n",
    "                csv_url = dir_url + csv_name\n",
    "                # Construct the local path of the extracted file\n",
    "                extracted_path = os.path.join(dir_name, csv_name[:-3])\n",
    "                # Check if the extracted file exists locally\n",
    "                if file_exists(extracted_path):\n",
    "                    print('Skipping download of', csv_name, '- extracted file already exists.')\n",
    "                else:\n",
    "                    # Construct the local path of the archive file\n",
    "                    archive_path = os.path.join(dir_name, csv_name)\n",
    "                    # Download the archive file if it doesn't exist locally\n",
    "                    if not file_exists(archive_path):\n",
    "                        download_file(csv_url, archive_path)\n",
    "                        print('Downloaded:', archive_path)\n",
    "                        time.sleep(0.1)\n",
    "                    # Check if the file is a gzip archive\n",
    "                    if csv_name.endswith('.gz'):\n",
    "                        # Open the gzip archive and extract the contents\n",
    "                        with gzip.open(archive_path, 'rb') as f_in:\n",
    "                            with open(extracted_path, 'wb') as f_out:\n",
    "                                f_out.write(f_in.read())\n",
    "                                print('Extracted:', extracted_path)\n",
    "                        # Remove the archive file\n",
    "                        os.remove(archive_path)\n",
    "                        print('Removed:', archive_path)\n",
    "                    else:\n",
    "                        # Rename the file to remove the .csv extension\n",
    "                        os.rename(archive_path, extracted_path)\n",
    "                        print('Renamed:', archive_path, 'to', extracted_path)\n",
    "            else:\n",
    "                # Skip the file\n",
    "                if csv_date < start_date:\n",
    "                    print('Skipping download of', csv_name, '- date is before start date.')\n",
    "                elif csv_date > end_date:\n",
    "                    print('Skipping download of', csv_name, '- date is after end date.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
